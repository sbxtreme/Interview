Mongodb datamodelling
----------------------

1) The documents are stored in mongodb in the form of bson which is binary json. BSON is a binary serialization format used to store documents and make remote procedure calls in MongoDB.The reason for storing in bson format is because it is lightweight and faster.

2) In case of RDBMS , the schemas are fixed where as in case of Nosql the schemas are flexible.In Mongodb the collections can store documents of different structures.


3) In case of MongoDB all the crud operations can be done using json.We just have to pass json in the mongodb functions. ex:
-- insert : db.emp.insert({"empname":"shobhit"})
-- select : db.emp.find({"empname":"shobhit","age":10})

4) Post 3.0 version, The max size of documents in Mongodb is 16mb but if documents are greater than 16mb still mongodb will store but it will store in the form of mongo files which is a binary file called as GRIDFS.

5) Below conventions must be followed in mongodb:
 a) the key names should not contain . i.e {"first.name":"shobhit"} # error
 b) the key should not contain $ i.e {"first$name":"shobhit"} # error

 . is used for traversing into the documents i.e
 {name:{first:"shobhit",last:"bhatnagar"}}
 so we can find the above document in mongodb using find() i.e db.collectionname.find({"name.first":"shobhit"})
A
$ is used with the operators like $gte,$and,$not etc..

6) In mongodb there is going to a primary key which will be autogenerated key using which we can identify the documents, which is "_id" field. This "_id" field is called as object id. We can also explicitly give the _id to our documents while inserting but if we don't give mongodb will automatically generate the id field.
There are 3 limitations for this field:
 a) It should not be null
 b) It should not be duplicate
 c) It should not of type array


Note:
-----

 In mongodb it supports 3 types of storage engines. They are :
 1) MMAPV1 
 --> entire collection will be locked and no updates will run on any of the document if someone is updating any 1 of the documents.
 2) WiredTiger 
 --> only a document which is modified will be locked but entire collection will not be locked. Due to this any other document can also be modified.
 3) InMemory 



7) There are 2 types of data modelling in Mongodb. They are:
 a) Embedded data modelling:
 --------------------------
 
	Let's take an example of student data , in this data we have details about students registration , student class/semesters and student exam details. In All the 3 cases student id will be same and there might be some field which will be common like class in class details and class in exam details , so instead of creating 3 different collections , we will create a single collection which will have embedded documents i.e in a single 
	document all the details will be present . Below are the benefits of embedded documents.

	-- No need to join different collections , a single find operation will give all the details about student.
	   also a single update will update the values of a document.

	-- Atomic operations can be applied at document level i.e either all will be updated or nothing will be updated, this will help to reduce data inconsistency where as if the collections are different then there might be chances that 1 collection document is updated while the other is left.

Disadvantage:
1) Size of documents will be more as all the details are embedded in a single document.
2) Changes of high redundancy.


 b) Reference data modelling:
 ----------------------------
 	This model is used for normalised data i.e don't keep the data in a single document instead keep the data into multiple documents i.e 1 document for student admin details , 1 for class details and 1 for exam details and join all the 3 documents using linked keys. _id by default is used to link 2 or more documents. 


In mongodb there are no joins but this can be achieved by using references.s
In Industry the embadded moduling is 70% and reference moduling is 30%.

Advantages of embadded moduling:
1) easy to read
2) easy to update 
3) less chances of data inconsistancy

Disadvantages:
1) Size of the document will be much more than reference documents as it is denormalize.
2) High redundency



GRIDFS:
-------
If the document is more than 16mb then GRIDFS will break the files into 256kb files i.e into chunks 
and store it in database. Here there are 2 parts, 1 which stores the data and 1 which stores the metadata that which file data is stored where.

There are going to be 2 collections:
1) Metadata
2) Actual data

There are 3 types of relations in mongodb data which can be maintained:
1) one to one ---- go with embedded documents ex:customer name and customer dept
2) one to many ---- go with embedded documents ex: player playing for different teams
3) many to many ---- go with linked documents as in case of embedded size will increase.


CAP theorem:
-------------
According to cap theorem any distributed database should choose 2 out of 3 features of CAP theorem i.e Consistancy , Availability and partition tolerance.


Availability -- even if any one node goes down , we can access the data.

Consistency -- every data that you read is the most recent

Partition tolerance -- tolerate network outage between nodes


Mongodb comes under consistency and partition tolerance. It has master and slave architechture i.e 1 node will provide data and other nodes will simply store the data.
if master node is down then election process will happen to chose any slave node as master node and till that time data will not be available.

Mongodb has consistency that is the latest data will be stored in all the nodes whether master or slave node and hence all the nodes have latest data ( no stale data ).

Mongodb is partition tolerance i.e the database supports sharding concept in which database is shared accross multiple nodes and in case of breakdown of any node , other node will be treated as master and will provide data.



Relational databases:
These databases provides consistency by having master ( prod ) and slave (DR) databases and the data is synced between these 2 databases 

Cassendra :
It supports Availability and partition tolerance i.e there is no master or slave concept.
All the nodes are available to provide data in case of breakdown of any node and it supports database sharding hence data is distributed accross all the nodes. If data updation needs to be done then it will start from node 1 and at that time if some one reads the data from node 2 then he/she will get stale ( old data ) as the data is not updated in node2.



Replicata sets in mongodb:
---------------------------
In a replicate set we have a primary server and multiple secondary servers.
The data will be synced from primary to secondary servers.
If we want to write data we have to use primary server. 
If we want to read data we can use any server either primary or secondary.


ACID properties:
----------------
Atomicity -- It means in a transaction either all the instructions participating in a transaction must execute or nothing should execute. This makes the data consistent and complete. ex: In a transaction if 2 updates are there so both should be run and data must be commited else if any one will run then it will make data inconsistent.

Consistent -- If a system is consistent before execution of transaction then it must remian consistent after the executionof transaction. ex: if a= 5 before transaction and i have added 1 and then it the final database state must be a+1=6 i.e the correct state of database should be there after the transaction. Data should be correct after transaction.

Isolation -- It means that if 2 or more transactions are executing seperately without affecting each other then it means that they are isolated.

Durability -- It means if a data is stored in database and in case of software or hardware failure if the value still remains same and is not impacted  then it shows that database is durable.


In order to increase the performance of mongodb :
1) We should use replication across all the slave nodes from master so that read operation  can refer any of the slave node . Data will be updated on slaves due to replication from master.

2) Sharding - It is a process of distributing data into multiple machines based on some attributes i.e if the total data is of 1000 users then we can shard the data based on userids. User 1-100 on 1 server , 101-200 on other and so on. Using this if someone wants to access 87 userid then the data scan should not be done on the complete 1000 users but instead it will be done on server 1 from 1-100 users. This will increase the performance of database reads.Also indexes can also be created on sharded servers on an attribute.

In sharding there are multiple replica sets on each servers.

server1(primary node --secondary node)-----server2(primary node --secondary node)------server3(primary node --secondary node)
